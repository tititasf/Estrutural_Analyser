
# Helper de ofuscação (adicionado automaticamente)
def _get_obf_str(key):
    """Retorna string ofuscada"""
    _obf_map = {
        _get_obf_str("script.google.com"): base64.b64decode("=02bj5SZsd2bvdmL0BXayN2c"[::-1].encode()).decode(),
        _get_obf_str("macros/s/"): base64.b64decode("vM3Lz9mcjFWb"[::-1].encode()).decode(),
        _get_obf_str("AKfycbz"): base64.b64decode("==geiNWemtUQ"[::-1].encode()).decode(),
        _get_obf_str("credit"): base64.b64decode("0lGZlJ3Y"[::-1].encode()).decode(),
        _get_obf_str("saldo"): base64.b64decode("=8GZsF2c"[::-1].encode()).decode(),
        _get_obf_str("consumo"): base64.b64decode("==wbtV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str("api_key"): base64.b64decode("==Qelt2XpBXY"[::-1].encode()).decode(),
        _get_obf_str("user_id"): base64.b64decode("==AZp9lclNXd"[::-1].encode()).decode(),
        _get_obf_str("calcular_creditos"): base64.b64decode("=M3b0lGZlJ3YfJXYsV3YsF2Y"[::-1].encode()).decode(),
        _get_obf_str("confirmar_consumo"): base64.b64decode("=8Wb1NnbvN2XyFWbylmZu92Y"[::-1].encode()).decode(),
        _get_obf_str("consultar_saldo"): base64.b64decode("vRGbhN3XyFGdsV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str("debitar_creditos"): base64.b64decode("==wcvRXakVmcj9lchRXaiVGZ"[::-1].encode()).decode(),
        _get_obf_str("CreditManager"): base64.b64decode("==gcldWYuFWT0lGZlJ3Q"[::-1].encode()).decode(),
        _get_obf_str("obter_hwid"): base64.b64decode("==AZpdHafJXZ0J2b"[::-1].encode()).decode(),
        _get_obf_str("generate_signature"): base64.b64decode("lJXd0Fmbnl2cfVGdhJXZuV2Z"[::-1].encode()).decode(),
        _get_obf_str("encrypt_string"): base64.b64decode("=cmbpJHdz9Fdwlncj5WZ"[::-1].encode()).decode(),
        _get_obf_str("decrypt_string"): base64.b64decode("=cmbpJHdz9FdwlncjVGZ"[::-1].encode()).decode(),
        _get_obf_str("integrity_check"): base64.b64decode("rNWZoN2X5RXaydWZ05Wa"[::-1].encode()).decode(),
        _get_obf_str("security_utils"): base64.b64decode("=MHbpRXdflHdpJXdjV2c"[::-1].encode()).decode(),
        _get_obf_str("https://"): base64.b64decode("=8yL6MHc0RHa"[::-1].encode()).decode(),
        _get_obf_str("google.com"): base64.b64decode("==QbvNmLlx2Zv92Z"[::-1].encode()).decode(),
        _get_obf_str("apps.script"): base64.b64decode("=QHcpJ3Yz5ycwBXY"[::-1].encode()).decode(),
    }
    return _obf_map.get(key, key)


# Helper de ofuscação (adicionado automaticamente)
def _get_obf_str(key):
    """Retorna string ofuscada"""
    _obf_map = {
        _get_obf_str(_get_obf_str("script.google.com")): base64.b64decode("=02bj5SZsd2bvdmL0BXayN2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("macros/s/")): base64.b64decode("vM3Lz9mcjFWb"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("AKfycbz")): base64.b64decode("==geiNWemtUQ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("credit")): base64.b64decode("0lGZlJ3Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("saldo")): base64.b64decode("=8GZsF2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("consumo")): base64.b64decode("==wbtV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("api_key")): base64.b64decode("==Qelt2XpBXY"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("user_id")): base64.b64decode("==AZp9lclNXd"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("calcular_creditos")): base64.b64decode("=M3b0lGZlJ3YfJXYsV3YsF2Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("confirmar_consumo")): base64.b64decode("=8Wb1NnbvN2XyFWbylmZu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("consultar_saldo")): base64.b64decode("vRGbhN3XyFGdsV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("debitar_creditos")): base64.b64decode("==wcvRXakVmcj9lchRXaiVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("CreditManager")): base64.b64decode("==gcldWYuFWT0lGZlJ3Q"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("obter_hwid")): base64.b64decode("==AZpdHafJXZ0J2b"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("generate_signature")): base64.b64decode("lJXd0Fmbnl2cfVGdhJXZuV2Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("encrypt_string")): base64.b64decode("=cmbpJHdz9Fdwlncj5WZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("decrypt_string")): base64.b64decode("=cmbpJHdz9FdwlncjVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("integrity_check")): base64.b64decode("rNWZoN2X5RXaydWZ05Wa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("security_utils")): base64.b64decode("=MHbpRXdflHdpJXdjV2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("https://")): base64.b64decode("=8yL6MHc0RHa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("google.com")): base64.b64decode("==QbvNmLlx2Zv92Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str("apps.script")): base64.b64decode("=QHcpJ3Yz5ycwBXY"[::-1].encode()).decode(),
    }
    return _obf_map.get(key, key)


# Helper de ofuscação (adicionado automaticamente)
def _get_obf_str(key):
    """Retorna string ofuscada"""
    _obf_map = {
        _get_obf_str(_get_obf_str(_get_obf_str("script.google.com"))): base64.b64decode("=02bj5SZsd2bvdmL0BXayN2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("macros/s/"))): base64.b64decode("vM3Lz9mcjFWb"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("AKfycbz"))): base64.b64decode("==geiNWemtUQ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("credit"))): base64.b64decode("0lGZlJ3Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("saldo"))): base64.b64decode("=8GZsF2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("consumo"))): base64.b64decode("==wbtV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("api_key"))): base64.b64decode("==Qelt2XpBXY"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("user_id"))): base64.b64decode("==AZp9lclNXd"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("calcular_creditos"))): base64.b64decode("=M3b0lGZlJ3YfJXYsV3YsF2Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("confirmar_consumo"))): base64.b64decode("=8Wb1NnbvN2XyFWbylmZu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("consultar_saldo"))): base64.b64decode("vRGbhN3XyFGdsV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("debitar_creditos"))): base64.b64decode("==wcvRXakVmcj9lchRXaiVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("CreditManager"))): base64.b64decode("==gcldWYuFWT0lGZlJ3Q"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("obter_hwid"))): base64.b64decode("==AZpdHafJXZ0J2b"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("generate_signature"))): base64.b64decode("lJXd0Fmbnl2cfVGdhJXZuV2Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("encrypt_string"))): base64.b64decode("=cmbpJHdz9Fdwlncj5WZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("decrypt_string"))): base64.b64decode("=cmbpJHdz9FdwlncjVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("integrity_check"))): base64.b64decode("rNWZoN2X5RXaydWZ05Wa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("security_utils"))): base64.b64decode("=MHbpRXdflHdpJXdjV2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("https://"))): base64.b64decode("=8yL6MHc0RHa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("google.com"))): base64.b64decode("==QbvNmLlx2Zv92Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str("apps.script"))): base64.b64decode("=QHcpJ3Yz5ycwBXY"[::-1].encode()).decode(),
    }
    return _obf_map.get(key, key)


# Helper de ofuscação (adicionado automaticamente)
def _get_obf_str(key):
    """Retorna string ofuscada"""
    _obf_map = {
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("script.google.com")))): base64.b64decode("=02bj5SZsd2bvdmL0BXayN2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("macros/s/")))): base64.b64decode("vM3Lz9mcjFWb"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("AKfycbz")))): base64.b64decode("==geiNWemtUQ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("credit")))): base64.b64decode("0lGZlJ3Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("saldo")))): base64.b64decode("=8GZsF2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("consumo")))): base64.b64decode("==wbtV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("api_key")))): base64.b64decode("==Qelt2XpBXY"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("user_id")))): base64.b64decode("==AZp9lclNXd"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("calcular_creditos")))): base64.b64decode("=M3b0lGZlJ3YfJXYsV3YsF2Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("confirmar_consumo")))): base64.b64decode("=8Wb1NnbvN2XyFWbylmZu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("consultar_saldo")))): base64.b64decode("vRGbhN3XyFGdsV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("debitar_creditos")))): base64.b64decode("==wcvRXakVmcj9lchRXaiVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("CreditManager")))): base64.b64decode("==gcldWYuFWT0lGZlJ3Q"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("obter_hwid")))): base64.b64decode("==AZpdHafJXZ0J2b"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("generate_signature")))): base64.b64decode("lJXd0Fmbnl2cfVGdhJXZuV2Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("encrypt_string")))): base64.b64decode("=cmbpJHdz9Fdwlncj5WZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("decrypt_string")))): base64.b64decode("=cmbpJHdz9FdwlncjVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("integrity_check")))): base64.b64decode("rNWZoN2X5RXaydWZ05Wa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("security_utils")))): base64.b64decode("=MHbpRXdflHdpJXdjV2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("https://")))): base64.b64decode("=8yL6MHc0RHa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("google.com")))): base64.b64decode("==QbvNmLlx2Zv92Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("apps.script")))): base64.b64decode("=QHcpJ3Yz5ycwBXY"[::-1].encode()).decode(),
    }
    return _obf_map.get(key, key)


# Helper de ofuscação (adicionado automaticamente)
def _get_obf_str(key):
    """Retorna string ofuscada"""
    _obf_map = {
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("script.google.com"))))): base64.b64decode("=02bj5SZsd2bvdmL0BXayN2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("macros/s/"))))): base64.b64decode("vM3Lz9mcjFWb"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("AKfycbz"))))): base64.b64decode("==geiNWemtUQ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("credit"))))): base64.b64decode("0lGZlJ3Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("saldo"))))): base64.b64decode("=8GZsF2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("consumo"))))): base64.b64decode("==wbtV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("api_key"))))): base64.b64decode("==Qelt2XpBXY"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("user_id"))))): base64.b64decode("==AZp9lclNXd"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("calcular_creditos"))))): base64.b64decode("=M3b0lGZlJ3YfJXYsV3YsF2Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("confirmar_consumo"))))): base64.b64decode("=8Wb1NnbvN2XyFWbylmZu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("consultar_saldo"))))): base64.b64decode("vRGbhN3XyFGdsV3cu92Y"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("debitar_creditos"))))): base64.b64decode("==wcvRXakVmcj9lchRXaiVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("CreditManager"))))): base64.b64decode("==gcldWYuFWT0lGZlJ3Q"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("obter_hwid"))))): base64.b64decode("==AZpdHafJXZ0J2b"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("generate_signature"))))): base64.b64decode("lJXd0Fmbnl2cfVGdhJXZuV2Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("encrypt_string"))))): base64.b64decode("=cmbpJHdz9Fdwlncj5WZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("decrypt_string"))))): base64.b64decode("=cmbpJHdz9FdwlncjVGZ"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("integrity_check"))))): base64.b64decode("rNWZoN2X5RXaydWZ05Wa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("security_utils"))))): base64.b64decode("=MHbpRXdflHdpJXdjV2c"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("https://"))))): base64.b64decode("=8yL6MHc0RHa"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("google.com"))))): base64.b64decode("==QbvNmLlx2Zv92Z"[::-1].encode()).decode(),
        _get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str(_get_obf_str("apps.script"))))): base64.b64decode("=QHcpJ3Yz5ycwBXY"[::-1].encode()).decode(),
    }
    return _obf_map.get(key, key)

import sqlite3
import json
import logging
from typing import List, Dict, Optional, Any
from pathlib import Path

class DatabaseManager:
    """
    Gerencia a persistência local dos dados do projeto usando SQLite.
    Salva o estado dos elementos identificados (Pilares, Vigas, Lajes).
    """
    def __init__(self, db_path: str = "project_data.vision"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """Cria as tabelas se não existirem."""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        self._create_tables_if_not_exist(cursor)

        # Migração: Verificar se colunas de projeto existem (caso a tabela já existisse sem elas)
        self._migrate_db(cursor)

        conn.commit()
        conn.close()

    def _create_tables_if_not_exist(self, cursor):
        """Define o schema das tabelas."""
        # Tabela de Obras (Persistência independente de projetos)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS works (
                name TEXT PRIMARY KEY,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # Tabela de Projetos
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS projects (
                id TEXT PRIMARY KEY,
                name TEXT,
                dxf_path TEXT,
                work_name TEXT,
                pavement_name TEXT,
                level_arrival TEXT,
                level_exit TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                author_name TEXT,
                sync_status TEXT DEFAULT 'pending'
            )
        ''')
        
        # Tabela de Eventos de Treinamento (Log para Active Learning)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS training_events (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                type TEXT,        -- 'manual_correction', 'auto_validation'
                role TEXT,        -- 'pillar_dim', 'beam_name'
                context_dna_json TEXT, -- Assinatura vetorial do momento
                target_value TEXT,     -- O valor correto (label)
                status TEXT,           -- 'valid', 'fail'
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')

        # Tabela de Pilares (Schema expandido para IA + Projeto)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pillars (
                id TEXT PRIMARY KEY, 
                project_id TEXT,
                name TEXT,
                type TEXT,
                area REAL,
                points_json TEXT,
                sides_data_json TEXT, 
                links_json TEXT, 
                conf_map_json TEXT, 
                validated_fields_json TEXT, 
                validated_link_classes_json TEXT,
                na_fields_json TEXT,
                na_link_classes_json TEXT,
                na_reasons_json TEXT,
                issues_json TEXT, 
                id_item TEXT,
                is_validated BOOLEAN DEFAULT 0,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')
        
        # Tabela de Lajes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS slabs (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                name TEXT,
                type TEXT DEFAULT 'Laje',
                area REAL,
                points_json TEXT,
                links_json TEXT,
                validated_fields_json TEXT,
                validated_link_classes_json TEXT,
                na_fields_json TEXT,
                na_link_classes_json TEXT,
                na_reasons_json TEXT,
                issues_json TEXT,
                id_item TEXT,
                is_validated BOOLEAN DEFAULT 0,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')

        # Tabela de Vigas
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS beams (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                name TEXT,
                data_json TEXT, 
                sides_data_json TEXT,
                links_json TEXT,
                validated_fields_json TEXT,
                validated_link_classes_json TEXT,
                na_fields_json TEXT,
                na_link_classes_json TEXT,
                na_reasons_json TEXT,
                issues_json TEXT,
                id_item TEXT,
                is_validated BOOLEAN DEFAULT 0,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')

        # Tabela de Pré-processamento (Marco DXF)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pre_processing (
                project_id TEXT PRIMARY KEY,
                data_json TEXT,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')
        
        # Tabela de Contornos (Marcos Separados)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS contours (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                name TEXT,
                data_json TEXT,
                is_validated BOOLEAN DEFAULT 0,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')

    def _migrate_db(self, cursor):
        """Adiciona colunas necessárias, corrige tipos e migra dados."""
        logging.info("Checking database migrations...")
        
        # Helper para verificar e adicionar colunas
        def _check_and_add_column(table_name, column_name, column_type):
            try:
                # Verificar se a coluna já existe
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = [info[1] for info in cursor.fetchall()]
                
                if column_name not in columns:
                    cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_type}")
                    logging.info(f"✅ Migração: Coluna '{column_name}' adicionada à tabela '{table_name}'.")
                else:
                    logging.debug(f"ℹ️ Coluna '{column_name}' já existe em '{table_name}'.")
            except Exception as e:
                logging.error(f"❌ Erro na migração ({table_name}.{column_name}): {e}")

        # Migração de Obras existentes
        try:
            cursor.execute("INSERT OR IGNORE INTO works (name) SELECT DISTINCT work_name FROM projects WHERE work_name IS NOT NULL AND work_name != ''")
        except Exception as e: pass

        # --- NOVAS COLUNAS ---
        
        # SLABS
        _check_and_add_column('slabs', 'type', "TEXT DEFAULT 'Laje'")
        _check_and_add_column('slabs', 'links_json', 'TEXT')
        _check_and_add_column('slabs', 'validated_fields_json', 'TEXT')
        _check_and_add_column('slabs', 'issues_json', 'TEXT')
        _check_and_add_column('slabs', 'is_validated', 'BOOLEAN DEFAULT 0')
        _check_and_add_column('slabs', 'id_item', 'TEXT')
        
        # PILLARS
        _check_and_add_column('pillars', 'id_item', 'TEXT')
        
        # BEAMS
        _check_and_add_column('beams', 'id_item', 'TEXT')
        _check_and_add_column('beams', 'sides_data_json', 'TEXT')
        _check_and_add_column('beams', 'links_json', 'TEXT')
        _check_and_add_column('beams', 'validated_fields_json', 'TEXT')
        _check_and_add_column('beams', 'issues_json', 'TEXT')
        _check_and_add_column('beams', 'is_validated', 'BOOLEAN DEFAULT 0')

        # PROJECTS
        _check_and_add_column('projects', 'last_sync_at', 'TIMESTAMP')

        # GLOBAL N/A AND VALIDATION COLUMNS FOR ALL CORE TABLES
        for table in ['pillars', 'beams', 'slabs']:
            _check_and_add_column(table, 'validated_link_classes_json', 'TEXT')
            _check_and_add_column(table, 'na_fields_json', 'TEXT')
            _check_and_add_column(table, 'na_link_classes_json', 'TEXT')
            _check_and_add_column(table, 'na_reasons_json', 'TEXT')

    def create_work(self, name: str):
        """Cria uma nova Obra vazia."""
        conn = self._get_conn()
        try:
            conn.execute('INSERT OR IGNORE INTO works (name) VALUES (?)', (name,))
            conn.commit()
        finally:
            conn.close()

    def rename_work(self, old_name: str, new_name: str):
        """Renomeia uma Obra e atualiza referências."""
        conn = self._get_conn()
        try:
            # 1. Update works table
            conn.execute('UPDATE works SET name = ? WHERE name = ?', (new_name, old_name))
            # 2. Update projects reference
            conn.execute('UPDATE projects SET work_name = ? WHERE work_name = ?', (new_name, old_name))
            conn.commit()
        finally:
            conn.close()

    def delete_work(self, name: str):
        """Exclui uma Obra (seus projetos ficam sem obra)."""
        conn = self._get_conn()
        try:
            # 1. Update projects to remove work ref
            conn.execute('UPDATE projects SET work_name = "" WHERE work_name = ?', (name,))
            # 2. Delete from works
            conn.execute('DELETE FROM works WHERE name = ?', (name,))
            conn.commit()
        finally:
            conn.close()

    def get_all_works(self) -> List[str]:
        """Retorna lista de todas as Obras cadastradas."""
        conn = self._get_conn()
        try:
            cursor = conn.execute('SELECT name FROM works ORDER BY name ASC')
            return [r[0] for r in cursor.fetchall()]
        finally:
            conn.close()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS training_events (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                type TEXT,        -- 'manual_correction', 'auto_validation'
                role TEXT,        -- 'pillar_dim', 'beam_name'
                context_dna_json TEXT, -- Assinatura vetorial do momento
                target_value TEXT,     -- O valor correto (label)
                status TEXT,           -- 'valid', 'fail'
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')

        # Tabela de Pilares (Schema expandido para IA + Projeto)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pillars (
                id TEXT PRIMARY KEY, 
                project_id TEXT,
                name TEXT,
                type TEXT,
                area REAL,
                points_json TEXT,
                sides_data_json TEXT, 
                links_json TEXT, 
                conf_map_json TEXT, 
                validated_fields_json TEXT, 
                issues_json TEXT, 
                is_validated BOOLEAN DEFAULT 0,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')
        
        # Tabela de Lajes
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS slabs (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                name TEXT,
                area REAL,
                points_json TEXT,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')

        # Tabela de Vigas
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS beams (
                id TEXT PRIMARY KEY,
                project_id TEXT,
                name TEXT,
                data_json TEXT, 
                is_validated BOOLEAN DEFAULT 0,
                FOREIGN KEY(project_id) REFERENCES projects(id)
            )
        ''')



        # 1. Verificar tipo da coluna ID na tabela pillars
        need_recreation = False
        try:
            cursor.execute("PRAGMA table_info(pillars)")
            cols = cursor.fetchall()  # [(cid, name, type, notnull, dflt_value, pk), ...]
            for c in cols:
                if c[1] == 'id' and 'INT' in c[2].upper():
                    logging.warning("Detectado esquema antigo (ID INTEGER). Tabelas serão recriadas.")
                    need_recreation = True
                    break
        except Exception:
            pass

        if need_recreation:
            tables = ['pillars', 'slabs', 'beams']
            for t in tables:
                try:
                    cursor.execute(f"ALTER TABLE {t} RENAME TO {t}_backup_legacy")
                except Exception as e:
                    logging.warning(f"Erro ao renomear {t}: {e}") # Talvez não exista?
            
            # As tabelas serão recriadas pelo _init_db (re-chamada ou fluxo seguinte)
            self._create_tables_if_not_exist(cursor) # Extrairemos esse método


        tables_to_check = {
            'pillars': ['project_id', 'links_json', 'conf_map_json', 'validated_fields_json', 'issues_json', 'sides_data_json', 'points_json'],
            'slabs': ['project_id'],
            'beams': ['project_id'],
            'projects': ['work_name', 'pavement_name', 'level_arrival', 'level_exit', 'author_name', 'sync_status']
        }
        
        for table, columns in tables_to_check.items():
            # (Código de verificação de colunas existente...)
            try:
                cursor.execute(f"PRAGMA table_info({table})")
                existing_cols = [col[1] for col in cursor.fetchall()]
                
                for col in columns:
                    if col not in existing_cols:
                        try:
                            cursor.execute(f"ALTER TABLE {table} ADD COLUMN {col} TEXT")
                            logging.info(f"Migration: Added column {col} to table {table}")
                        except Exception as e:
                            logging.error(f"Migration error on {table}.{col}: {e}")
            except:
                pass 

    def create_work(self, name: str):
        """Cria uma nova Obra vazia."""
        conn = self._get_conn()
        try:
            conn.execute('INSERT OR IGNORE INTO works (name) VALUES (?)', (name,))
            conn.commit()
        finally:
            conn.close()

    def rename_work(self, old_name: str, new_name: str):
        """Renomeia uma Obra e atualiza referências."""
        conn = self._get_conn()
        try:
            # 1. Update works table
            conn.execute('UPDATE works SET name = ? WHERE name = ?', (new_name, old_name))
            # 2. Update projects reference
            conn.execute('UPDATE projects SET work_name = ? WHERE work_name = ?', (new_name, old_name))
            conn.commit()
        finally:
            conn.close()

    def delete_work(self, name: str):
        """Exclui uma Obra (seus projetos ficam sem obra)."""
        conn = self._get_conn()
        try:
            # 1. Update projects to remove work ref
            conn.execute('UPDATE projects SET work_name = "" WHERE work_name = ?', (name,))
            # 2. Delete from works
            conn.execute('DELETE FROM works WHERE name = ?', (name,))
            conn.commit()
        finally:
            conn.close()

    def get_all_works(self) -> List[str]:
        """Retorna lista de todas as Obras cadastradas."""
        conn = self._get_conn()
        try:
            cursor = conn.execute('SELECT name FROM works ORDER BY name ASC')
            return [r[0] for r in cursor.fetchall()]
        finally:
            conn.close()

    def update_project_metadata(self, pid: str, metadata: Dict):
        """Atualiza metadados do projeto (Obra, Pavimento, Níveis)"""
        conn = self._get_conn()
        try:
            conn.execute('''
                UPDATE projects 
                SET work_name=?, pavement_name=?, level_arrival=?, level_exit=?, updated_at=CURRENT_TIMESTAMP
                WHERE id=?
            ''', (metadata.get('work_name'), metadata.get('pavement_name'), 
                  metadata.get('level_arrival'), metadata.get('level_exit'), pid))
            conn.commit()
        except Exception as e:
            logging.error(f"Failed to update metadata for {pid}: {e}")
        finally:
            conn.close() 

    def _get_conn(self):
        return sqlite3.connect(self.db_path)

    def create_project(self, name: str, dxf_path: str, author_name: str = "Local") -> str:
        """Cria novo projeto e retorna ID."""
        import uuid
        project_id = str(uuid.uuid4())
        conn = self._get_conn()
        try:
            conn.execute('INSERT INTO projects (id, name, dxf_path, author_name, sync_status) VALUES (?, ?, ?, ?, ?)', 
                        (project_id, name, dxf_path, author_name, 'pending'))
            conn.commit()
            return project_id
        except Exception as e:
            logging.error(f"Erro criar projeto: {e}")
            return None
        finally:
            conn.close()

    def get_projects(self) -> List[Dict]:
        """Lista projetos recentes com estatísticas de itens."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = """
            SELECT p.*,
                (SELECT COUNT(*) FROM pillars WHERE project_id = p.id) as pil_total,
                (SELECT COUNT(*) FROM pillars WHERE project_id = p.id AND is_validated=1) as pil_valid,
                (SELECT COUNT(*) FROM beams WHERE project_id = p.id) as beam_total,
                (SELECT COUNT(*) FROM beams WHERE project_id = p.id AND is_validated=1) as beam_valid,
                (SELECT COUNT(*) FROM slabs WHERE project_id = p.id) as slab_total,
                (SELECT COUNT(*) FROM slabs WHERE project_id = p.id AND is_validated=1) as slab_valid
            FROM projects p 
            ORDER BY updated_at DESC
            """
            cursor = conn.execute(sql)
            return [dict(r) for r in cursor.fetchall()]
        finally:
            conn.close()

    def get_project_by_id(self, project_id: str) -> Optional[Dict]:
        """Recupera um projeto pelo ID."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            cursor = conn.execute('SELECT * FROM projects WHERE id = ?', (project_id,))
            row = cursor.fetchone()
            return dict(row) if row else None
        finally:
            conn.close()

    def get_project_by_dxf_path(self, dxf_path: str) -> Optional[Dict]:
        """Recupera um projeto pelo caminho do DXF."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            cursor = conn.execute('SELECT * FROM projects WHERE dxf_path = ?', (dxf_path,))
            row = cursor.fetchone()
            return dict(row) if row else None
        finally:
            conn.close()

    def update_project_sync_status(self, project_id: str, status: str):
        """Atualiza estado de sincronização e timestamp de último sync."""
        conn = self._get_conn()
        try:
            # Se status for synced, atualiza last_sync_at
            if status == 'synced':
                 cur = conn.execute('UPDATE projects SET sync_status = ?, last_sync_at = CURRENT_TIMESTAMP WHERE id = ?', (status, project_id))
            else:
                 # Se for pending (ex: editado), apenas muda status
                 cur = conn.execute('UPDATE projects SET sync_status = ? WHERE id = ?', (status, project_id))
            
            if cur.rowcount == 0:
                logging.warning(f"⚠️ update_project_sync_status: Nenhuma linha afetada para ID {project_id} (Status: {status}). Verifique se o ID existe.")
            else:
                logging.info(f"✅ update_project_sync_status: {cur.rowcount} linhas atualizadas para ID {project_id}.")

            conn.commit()
        except Exception as e:
            logging.error(f"Erro ao atualizar status de sync do projeto {project_id}: {e}")
        finally:
            conn.close()

    def export_project_data(self, project_id: str) -> Optional[Dict]:
        """Empacota todos os dados de um projeto para sincronização/backup."""
        project = self.get_project_by_id(project_id)
        if not project: return None
        
        return {
            "project": project,
            "pillars": self.load_pillars(project_id),
            "beams": self.load_beams(project_id),
            "slabs": self.load_slabs(project_id),
            "training": self.get_training_events(project_id)
        }

    def import_project_data(self, data: Dict) -> Optional[str]:
        """Importa um pacote de dados completo para o banco local."""
        p_info = data.get('project')
        if not p_info: return None
        
        # Gera novo ID para evitar conflitos em importações externas
        new_id = self.create_project(
            p_info['name'], 
            p_info['dxf_path'], 
            p_info.get('author_name', 'Imported')
        )
        
        if not new_id: return None

        # Atualiza metadados
        self.update_project_metadata(new_id, {
            'work_name': p_info.get('work_name'),
            'pavement_name': p_info.get('pavement_name'),
            'level_arrival': p_info.get('level_arrival'),
            'level_exit': p_info.get('level_exit')
        })

        # Salva itens vinculados ao novo ID
        for p in data.get('pillars', []):
            p_copy = p.copy()
            if 'id' in p_copy: del p_copy['id'] # Garante novo ID local se necessário, ou deixa o save_pillar lidar
            self.save_pillar(p_copy, new_id)
            
        for b in data.get('beams', []):
            b_copy = b.copy()
            if 'id' in b_copy: del b_copy['id']
            self.save_beam(b_copy, new_id)
            
        for s in data.get('slabs', []):
            s_copy = s.copy()
            if 'id' in s_copy: del s_copy['id']
            self.save_slab(s_copy, new_id)
            
        for t in data.get('training', []):
            t_copy = t.copy()
            t_copy['project_id'] = new_id
            if 'id' in t_copy: del t_copy['id']
            self.log_training_event(t_copy)
        
        return new_id

    def log_training_event(self, event_data: Dict):
        """Registra um evento de treinamento."""
        import uuid
        conn = self._get_conn()
        try:
            conn.execute('''
                INSERT INTO training_events (id, project_id, type, role, context_dna_json, target_value, status)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                str(uuid.uuid4()),
                event_data.get('project_id'),
                event_data.get('type'),
                event_data.get('role'),
                json.dumps(event_data.get('dna', [])),
                event_data.get('value'),
                event_data.get('status')
            ))
            conn.commit()
        except Exception as e:
            logging.error(f"Erro log treino: {e}")
        finally:
            conn.close()

    def get_training_events(self, project_id: str) -> List[Dict]:
        """Recupera eventos de treino para um projeto."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            cursor = conn.execute('SELECT * FROM training_events WHERE project_id = ? ORDER BY timestamp DESC', (project_id,))
            return [dict(r) for r in cursor.fetchall()]
        except Exception as e:
            logging.error(f"Erro ao buscar eventos: {e}")
            return []
        finally:
            conn.close()

    def save_pillar(self, p: Dict[str, Any], project_id: str):
        """Salva ou atualiza um pilar (UPSERT) vinculado a um projeto."""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        try:
            # Serialização segura
            points_json = json.dumps(p.get('points', []))
            sides_json = json.dumps(p.get('sides_data', {}))
            links_json = json.dumps(p.get('links', {}))
            conf_map_json = json.dumps(p.get('confidence_map', {}))
            val_fields_json = json.dumps(p.get('validated_fields', []))
            val_links_json = json.dumps(p.get('validated_link_classes', {}))
            na_fields_json = json.dumps(p.get('na_fields', []))
            na_links_json = json.dumps(p.get('na_link_classes', {}))
            na_reasons_json = json.dumps(p.get('na_reasons', {}))
            issues_json = json.dumps(p.get('issues', []))
            
            p_id = str(p.get('id', ''))
            
            cursor.execute('''
                INSERT INTO pillars (
                    id, project_id, name, type, area, points_json, sides_data_json, 
                    links_json, conf_map_json, validated_fields_json, validated_link_classes_json,
                    na_fields_json, na_link_classes_json, na_reasons_json,
                    issues_json, id_item, is_validated
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(id) DO UPDATE SET
                    project_id=excluded.project_id,
                    name=excluded.name,
                    type=excluded.type,
                    area=excluded.area,
                    sides_data_json=excluded.sides_data_json,
                    links_json=excluded.links_json,
                    conf_map_json=excluded.conf_map_json,
                    validated_fields_json=excluded.validated_fields_json,
                    validated_link_classes_json=excluded.validated_link_classes_json,
                    na_fields_json=excluded.na_fields_json,
                    na_link_classes_json=excluded.na_link_classes_json,
                    na_reasons_json=excluded.na_reasons_json,
                    issues_json=excluded.issues_json,
                    id_item=excluded.id_item,
                    is_validated=excluded.is_validated
            ''', (
                p_id,
                project_id,
                p.get('name'), 
                p.get('type'), 
                float(p.get('area_val', 0.0)), 
                points_json, 
                sides_json,
                links_json,
                conf_map_json,
                val_fields_json,
                val_links_json,
                na_fields_json,
                na_links_json,
                na_reasons_json,
                issues_json,
                p.get('id_item'),
                1 if p.get('is_validated') else 0
            ))
            
            conn.commit()
        except Exception as e:
            logging.error(f"Erro ao salvar pilar no DB: {e}")
        finally:
            conn.close()

    def load_pillars(self, project_id: str) -> List[Dict]:
        """Carrega todos os pilares de um projeto."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        pillars = []
        try:
            cursor.execute('SELECT * FROM pillars WHERE project_id = ?', (project_id,))
            rows = cursor.fetchall()
            
            for row in rows:
                p = dict(row)
                p['points'] = json.loads(p.get('points_json') or '[]')
                p['sides_data'] = json.loads(p.get('sides_data_json') or '{}')
                p['links'] = json.loads(p.get('links_json') or '{}')
                p['confidence_map'] = json.loads(p.get('conf_map_json') or '{}')
                p['validated_fields'] = json.loads(p.get('validated_fields_json') or '[]')
                p['validated_link_classes'] = json.loads(p.get('validated_link_classes_json') or '{}')
                p['na_fields'] = json.loads(p.get('na_fields_json') or '[]')
                p['na_link_classes'] = json.loads(p.get('na_link_classes_json') or '{}')
                p['na_reasons'] = json.loads(p.get('na_reasons_json') or '{}')
                p['issues'] = json.loads(p.get('issues_json') or '[]')
                p['id_item'] = p.get('id_item')
                p['is_validated'] = bool(p['is_validated'])
                pillars.append(p)
        except Exception as e:
            logging.error(f"Erro ao carregar pilares: {e}")
        finally:
            conn.close()
        return pillars

    def delete_pillar(self, pillar_id: str):
        """Exclui um pilar específico pelo ID."""
        conn = self._get_conn()
        try:
            conn.execute("DELETE FROM pillars WHERE id = ?", (pillar_id,))
            conn.commit()
        finally:
            conn.close()

    def clear_project(self, project_id: str):
        """Limpa dados de um projeto específico."""
        conn = self._get_conn()
        cursor = conn.cursor()
        cursor.execute('DELETE FROM pillars WHERE project_id = ?', (project_id,))
        cursor.execute('DELETE FROM slabs WHERE project_id = ?', (project_id,))
        cursor.execute('DELETE FROM beams WHERE project_id = ?', (project_id,))
        conn.commit()
        conn.close()

    def save_slab(self, s: Dict[str, Any], project_id: str):
        """Salva uma laje vinculada ao projeto."""
        conn = self._get_conn()
        cursor = conn.cursor()
        try:
            cursor.execute('''
                INSERT INTO slabs (
                    id, project_id, name, type, area, points_json, 
                    links_json, validated_fields_json, validated_link_classes_json,
                    na_fields_json, na_link_classes_json, na_reasons_json,
                    issues_json, id_item, is_validated
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(id) DO UPDATE SET
                    project_id=excluded.project_id,
                    name=excluded.name,
                    type=excluded.type,
                    area=excluded.area,
                    points_json=excluded.points_json,
                    links_json=excluded.links_json,
                    validated_fields_json=excluded.validated_fields_json,
                    validated_link_classes_json=excluded.validated_link_classes_json,
                    na_fields_json=excluded.na_fields_json,
                    na_link_classes_json=excluded.na_link_classes_json,
                    na_reasons_json=excluded.na_reasons_json,
                    issues_json=excluded.issues_json,
                    id_item=excluded.id_item,
                    is_validated=excluded.is_validated
            ''', (
                s['id'], project_id, s.get('name'), 
                s.get('type', 'Laje'),
                float(s.get('area', 0.0)), 
                json.dumps(s.get('points', [])),
                json.dumps(s.get('links', {})),
                json.dumps(s.get('validated_fields', [])),
                json.dumps(s.get('validated_link_classes', {})),
                json.dumps(s.get('na_fields', [])),
                json.dumps(s.get('na_link_classes', {})),
                json.dumps(s.get('na_reasons', {})),
                json.dumps(s.get('issues', [])),
                s.get('id_item'),
                1 if s.get('is_validated') else 0
            ))
            
            conn.commit()
        except Exception as e:
            logging.error(f"Erro ao salvar laje no DB: {e}")
        finally:
            conn.close()



    def load_slabs(self, project_id: str) -> List[Dict]:
        """Carrega lajes de um projeto."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        slabs = []
        try:
            cursor.execute('SELECT * FROM slabs WHERE project_id = ?', (project_id,))
            for row in cursor.fetchall():
                s = dict(row)
                s['points'] = json.loads(s.get('points_json') or '[]')
                s['links'] = json.loads(s.get('links_json') or '{}')
                s['validated_fields'] = json.loads(s.get('validated_fields_json') or '[]')
                s['validated_link_classes'] = json.loads(s.get('validated_link_classes_json') or '{}')
                s['na_fields'] = json.loads(s.get('na_fields_json') or '[]')
                s['na_link_classes'] = json.loads(s.get('na_link_classes_json') or '{}')
                s['na_reasons'] = json.loads(s.get('na_reasons_json') or '{}')
                s['issues'] = json.loads(s.get('issues_json') or '[]')
                s['id_item'] = s.get('id_item')
                s['is_validated'] = bool(s.get('is_validated', 0))
                slabs.append(s)
        except Exception as e:
            logging.error(f"Erro ao carregar lajes: {e}")
        finally:
            conn.close()
        return slabs

    def delete_slab(self, slab_id: str):
        """Exclui uma laje específica pelo ID."""
        conn = self._get_conn()
        try:
            conn.execute("DELETE FROM slabs WHERE id = ?", (slab_id,))
            conn.commit()
        finally:
            conn.close()

    def save_beam(self, b: Dict[str, Any], project_id: str):
        """Salva uma viga vinculada ao projeto."""
        conn = self._get_conn()
        cursor = conn.cursor()
        try:
            cursor.execute('''
                INSERT INTO beams (
                    id, project_id, name, data_json, 
                    validated_fields_json, validated_link_classes_json,
                    na_fields_json, na_link_classes_json, na_reasons_json,
                    id_item, is_validated
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(id) DO UPDATE SET
                    project_id=excluded.project_id,
                    name=excluded.name,
                    data_json=excluded.data_json,
                    validated_fields_json=excluded.validated_fields_json,
                    validated_link_classes_json=excluded.validated_link_classes_json,
                    na_fields_json=excluded.na_fields_json,
                    na_link_classes_json=excluded.na_link_classes_json,
                    na_reasons_json=excluded.na_reasons_json,
                    id_item=excluded.id_item,
                    is_validated=excluded.is_validated
            ''', (
                b['id'], project_id, b.get('name'), 
                json.dumps(b), 
                json.dumps(b.get('validated_fields', [])),
                json.dumps(b.get('validated_link_classes', {})),
                json.dumps(b.get('na_fields', [])),
                json.dumps(b.get('na_link_classes', {})),
                json.dumps(b.get('na_reasons', {})),
                b.get('id_item'), 
                1 if b.get('is_validated') else 0
            ))
            conn.commit()
        except Exception as e:
            logging.error(f"Erro ao salvar viga: {e}")
        finally:
            conn.close()

    def load_beams(self, project_id: str) -> List[Dict]:
        """Carrega vigas de um projeto."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        beams = []
        try:
            cursor.execute('SELECT * FROM beams WHERE project_id = ?', (project_id,))
            for row in cursor.fetchall():
                b = json.loads(row['data_json'])
                # Sincronizar com colunas caso existam (Retrocompatibilidade)
                try:
                    b['is_validated'] = bool(row['is_validated'])
                except (IndexError, KeyError):
                    b['is_validated'] = b.get('is_validated', False)
                    
                try:
                    b['id_item'] = row['id_item']
                except (IndexError, KeyError):
                    b['id_item'] = b.get('id_item')

                # Carregar campos de validação/NA das colunas dedicadas
                for col, key in [('validated_fields_json', 'validated_fields'),
                                 ('validated_link_classes_json', 'validated_link_classes'),
                                 ('na_fields_json', 'na_fields'),
                                 ('na_link_classes_json', 'na_link_classes'),
                                 ('na_reasons_json', 'na_reasons')]:
                    try:
                        if row[col]: b[key] = json.loads(row[col])
                    except (IndexError, KeyError, TypeError):
                        pass
                
                beams.append(b)
        except Exception as e:
            logging.error(f"Erro ao carregar vigas: {e}")
        finally:
            conn.close()
        return beams

    def delete_beam(self, beam_id: str):
        """Exclui uma viga específica pelo ID."""
        conn = self._get_conn()
        try:
            conn.execute("DELETE FROM beams WHERE id = ?", (beam_id,))
            conn.commit()
        finally:
            conn.close()
    def save_pre_processing(self, project_id: str, data: Dict):
        """Salva dados de tratamento prévio (Marco)."""
        conn = self._get_conn()
        try:
            conn.execute('''
                INSERT INTO pre_processing (project_id, data_json, updated_at)
                VALUES (?, ?, CURRENT_TIMESTAMP)
                ON CONFLICT(project_id) DO UPDATE SET
                    data_json=excluded.data_json,
                    updated_at=excluded.updated_at
            ''', (project_id, json.dumps(data)))
            conn.commit()
        finally:
            conn.close()

    def load_pre_processing(self, project_id: str) -> Optional[Dict]:
        """Carrega dados de tratamento prévio."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            cursor = conn.cursor()
            cursor.execute('SELECT data_json FROM pre_processing WHERE project_id = ?', (project_id,))
            row = cursor.fetchone()
            return json.loads(row['data_json']) if row else None
        finally:
            conn.close()

    def export_project_data(self, project_id: str) -> Dict[str, Any]:
        """Exporta TODOS os dados de um projeto para um dicionário (Backup/Sharing)."""
        data = {}
        
        # 1. Project Info
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        try:
            cursor.execute("SELECT * FROM projects WHERE id = ?", (project_id,))
            row = cursor.fetchone()
            if row:
                data['project'] = dict(row)
            else:
                return {} # Projeto não encontrado
        finally:
            conn.close()

        # 2. Entities
        data['pillars'] = self.load_pillars(project_id)
        data['slabs'] = self.load_slabs(project_id)
        data['beams'] = self.load_beams(project_id)
        data['training'] = self.get_training_events(project_id)
        
        return data

    def import_project_data(self, data: Dict[str, Any]) -> str:
        """Importa dados de projeto. Atualiza se existir, cria se não."""
        p_info = data.get('project')
        if not p_info: return None
        
        p_id = p_info['id']
        conn = self._get_conn()
        cursor = conn.cursor()
        
        try:
            # 1. Upsert Project (Including Metadata)
            cursor.execute('''
                INSERT INTO projects (id, name, dxf_path, work_name, pavement_name, level_arrival, level_exit, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(id) DO UPDATE SET
                    name=excluded.name,
                    dxf_path=excluded.dxf_path,
                    work_name=excluded.work_name,
                    pavement_name=excluded.pavement_name,
                    level_arrival=excluded.level_arrival,
                    level_exit=excluded.level_exit
            ''', (
                p_id, 
                p_info['name'], 
                p_info['dxf_path'],
                p_info.get('work_name', ''),
                p_info.get('pavement_name', ''),
                p_info.get('level_arrival', ''),
                p_info.get('level_exit', ''),
                p_info.get('created_at')
            ))
            
            conn.commit() # Commit parcial para garantir ID
            
        except Exception as e:
            logging.error(f"Erro ao importar tabela projeto: {e}")
            return None
        finally:
            conn.close()
            
        # 2. Upsert Entities (Usando métodos existentes)
        for p in data.get('pillars', []):
            self.save_pillar(p, p_id)
            
        for s in data.get('slabs', []):
            self.save_slab(s, p_id)
            
        for b in data.get('beams', []):
            self.save_beam(b, p_id)

        # 3. Upsert Training Events
        self._import_training_events(data.get('training', []))
        
        return p_id

    def _import_training_events(self, events: List[Dict]):
        if not events: return
        conn = self._get_conn()
        cursor = conn.cursor()
        try:
            for e in events:
                keys = list(e.keys())
                vals = list(e.values())
                placeholders = ','.join(['?']*len(keys))
                cols = ','.join(keys)
                
                # Tenta INSERT OR IGNORE se tiver ID
                sql = f"INSERT OR IGNORE INTO training_events ({cols}) VALUES ({placeholders})"
                cursor.execute(sql, vals)
            conn.commit()
        except Exception as e:
            logging.error(f"Erro ao importar eventos de treino: {e}")
        finally:
            conn.close()

    def delete_project_fully(self, project_id: str):
        """Remove completamente um projeto e seus dados."""
        self.clear_project(project_id) # Remove entities
        
        conn = self._get_conn()
        try:
            conn.execute('DELETE FROM training_events WHERE project_id = ?', (project_id,))
            conn.execute('DELETE FROM projects WHERE id = ?', (project_id,))
            conn.commit()
        finally:
            conn.close()

    def rename_project(self, project_id: str, new_name: str):
        """Renomeia um projeto."""
        conn = self._get_conn()
        try:
            conn.execute('UPDATE projects SET name = ? WHERE id = ?', (new_name, project_id))
            conn.commit()
        finally:
            conn.close()

    def update_project_work(self, project_id: str, new_work_name: str):
        """Move projeto para outra Obra."""
        conn = self._get_conn()
        try:
            conn.execute('UPDATE projects SET work_name = ? WHERE id = ?', (new_work_name, project_id))
            conn.commit()
        finally:
            conn.close()


    def duplicate_project(self, source_pid: str, target_work_name: str = None) -> str:
        """Duplica um projeto existente."""
        data = self.export_project_data(source_pid)
        if not data: return None
        
        import uuid
        new_id = str(uuid.uuid4())
        
        # Modify ID and Metadata for new project
        data['project']['id'] = new_id
        data['project']['name'] = f"{data['project']['name']} (Cópia)"
        if target_work_name is not None:
             data['project']['work_name'] = target_work_name
             
        # Import as new
        return self.import_project_data(data)

    def log_training_event(self, project_id, type, role, context_dna, target_value, status):
        """Registra um evento de aprendizado Hierárquico."""
        conn = self._get_conn()
        try:
            import uuid
            evt_id = str(uuid.uuid4())
            conn.execute('''
                INSERT INTO training_events (id, project_id, type, role, context_dna_json, target_value, status)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (evt_id, project_id, type, role, context_dna, target_value, status))
            conn.commit()
        except Exception as e:
            logging.error(f"Erro ao logar training_event: {e}")
        finally:
            conn.close()

    def get_training_events(self, project_id: str) -> List[Dict]:
        """Retorna todos os eventos de treino (valid e fail) para o projeto."""
        conn = self._get_conn()
        try:
            # Join with projects to get work name if needed, but here we focus on raw events
            cursor = conn.execute('''
                SELECT id, type, role, target_value, status, timestamp, context_dna_json 
                FROM training_events 
                WHERE project_id = ? 
                ORDER BY timestamp DESC
            ''', (project_id,))
            
            columns = [c[0] for c in cursor.description]
            results = []
            for row in cursor.fetchall():
                results.append(dict(zip(columns, row)))
            return results
        finally:
            conn.close()

    def delete_training_event(self, event_id: str):
        """Remove um evento de treino específico (Correção de Erro Humano)."""
        conn = self._get_conn()
        try:
            conn.execute('DELETE FROM training_events WHERE id = ?', (event_id,))
            conn.commit()
            logging.info(f"🗑️ Evento de treino {event_id} removido.")
        finally:
            conn.close()
    # --- ADMIN STATS ---
    
    def get_admin_stats(self) -> Dict[str, Any]:
        """Retorna estatísticas globais para o dashboard admin."""
        conn = self._get_conn()
        try:
            stats = {}
            stats['total_works'] = conn.execute("SELECT COUNT(DISTINCT work_name) FROM projects").fetchone()[0]
            stats['total_projects'] = conn.execute("SELECT COUNT(*) FROM projects").fetchone()[0]
            stats['total_pillars'] = conn.execute("SELECT COUNT(*) FROM pillars").fetchone()[0]
            stats['total_beams'] = conn.execute("SELECT COUNT(*) FROM beams").fetchone()[0]
            stats['total_slabs'] = conn.execute("SELECT COUNT(*) FROM slabs").fetchone()[0]
            
            # Entidades validadas vs total
            stats['valid_pillars'] = conn.execute("SELECT COUNT(*) FROM pillars WHERE is_validated=1").fetchone()[0]
            stats['valid_beams'] = conn.execute("SELECT COUNT(*) FROM beams WHERE is_validated=1").fetchone()[0]
            stats['valid_slabs'] = conn.execute("SELECT COUNT(*) FROM slabs WHERE is_validated=1").fetchone()[0]
            
            return stats
        except Exception as e:
            logging.error(f"Erro ao buscar admin stats: {e}")
            return {}
        finally:
            conn.close()

    def get_accuracy_report(self) -> List[Dict]:
        """Gera relatório de precisão baseado em eventos de treino."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = """
                SELECT role, status, COUNT(*) as count 
                FROM training_events 
                GROUP BY role, status
            """
            cursor = conn.execute(sql)
            return [dict(r) for r in cursor.fetchall()]
        except Exception as e:
            logging.error(f"Erro ao buscar accuracy report: {e}")
            return []
        finally:
            conn.close()
